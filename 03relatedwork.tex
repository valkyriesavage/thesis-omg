\chapter{Related Work}

\begin{quote}
If I have seen further, it is only by standing upon the shoulders of giants.

--- Isaac Newton
\end{quote}

Situating this thesis in the realm of prior work, we draw heavily on work from four major traditions: simulation, sensing, 3D modeling, and prototyping tools.

\section{Simulation}

    The simulation community was one of the first to rally around digital fabrication: they have delved extensively into digital simulation of machine toolpaths, materials and behaviors, and the advent of hyper-precise 3D printers like the Objet Connex line\footnote{At time of publication, the Connex3 allowed for XY resoltion of $600$dpi, Z resolution of $1600$dpi, and accuracy of 20-85 microns. \url{http://www.stratasys.com/~/media/Main/Files/Machine_Spec_Sheets/PSS_PJ_Connex3.ashx}} allowed for a variety of real-world manifestations of these simulations.

    \subsection{Early Simulation : Structures and Manufacturability}

    Mathematical simulation for structural analysis existed before digital fabrication, e.g. \cite{fleury-optimization}, and supported CNC machining. Some of the very earliest work on fabrication-related simulation focused on optimizing toolpaths, for example to ensure good coverage from robotic spray heads without occlusion from model features \cite{gursoz-noodles}. Other early work examined and accounted for deformities in models produced using different 3D printing technologies \cite{brown-simulate,hsu-numerical}, or ensuring printability of 3D models \cite{barequet-gaps,bohn-shellclosure}.

    \subsection{Materials and Behaviors}

    Early simulations paved the way for more reliable machines and quality outputs. With these basic properties nailed down, more recent visual simulation (i.e., graphics) research has explored pre-fabrication simulation techniques that affect the post-fabrication material characteristics of models.

    Among visible characteristics, work has been done on subsurface scattering \cite{hasan-subsurface} and subsurface reflectance \cite{weyrich-reflectance}; these techniques require using multi-material machines which can blend transparent and opaque materials. Such precision fabrication machines can also allow for creating tiny structural elements, like honeycombs, that can contribute to an object's overall appearance \cite{lan-appearance}. Another system can take motion capture sequences and translate them into fabricatable mechanical automata that perform the same motions, thanks to careful simulation of linkages \cite{ceylan-automata}.

    For tactile, audio, and tangible characteristics, Bickel, et al., model and fabricate particular deformation behaviors using optimized blends of rigid and soft materials \cite{bickel-deformation}. Higher-level optimization strategies can yield models with voids inside to ensure balance after fabrication \cite{prevost-makeitstand}, or musical instruments which have specific pitches after fabrication \cite{umetani-metallophone}. Other systems can infer the joints in digital character models, and add in printable, poseable joints which users can manipulate post-print \cite{bacher-posable,cali-articulated}.

    \subsection{Fabrication Itself}

    The actual fabrication process can also be improved with recent work in graphics. For example, linking knowledge of the layered manufacturing technique employed by FFF machines (described in Chapter 2) to a 3D model can help ensure that it will not break under force \cite{umetani-strength}, or that fabrication will produce minimal waste \cite{schmidt-support}. Closed-loop computer vision systems integrated with 3D printers can also improve fabrication accuracy \cite{sithi-multifab}.

    This thesis will draw on the same theme of pre-fabrication simulation of materials and models. Certain types of sensing necessitate guaranteeing that a prototype will have specific material properties, however our focus does not end with the fabricated object itself. We use information from the model to inform our sensing techniques and to develop interactive prototypes.

\section{3D CAD Tools}
    Tools for 3D modeling are becoming more useful and usable as 3D printers become more accessible. Powerful industrial modeling tools, like SolidWorks \cite{solidworks}, AutoCAD \cite{autocad}, ProEngineer \cite{proe}, and Rhino \cite{rhino}, have existed for years and served industrial designers and professional engineers very well. Makers find lower-threshold, lower-ceiling tools like Meshmixer \cite{schmidt-meshmixer} suitable for their needs. However, research has investigated more accessible ways of modeling objects.

    \subsection{Domain-Specific Constraints}

    A few research projects have investigated domain-specific constraints into 3D modeling tools. In general, this allows for smart feedback to users, as well as pre-fabrication simulations. For example, the Plushie system \cite{mori-plushie} allows users to design 3D fabricatable teddy bears using only 2D sketches: the system has a basis for what constitutes a teddy bear, and can infer the third dimension. It even presents a preview to users as they author \valkyrie{is that even true?}. SketchChair also allows users to create 3D objects from 2D sketches using such constraints: in this case, users sketch chairs in profile \cite{saul-sketchchair}. This system allows users to simulate bodies of various sizes and weights sitting in the chairs to ensure comfort and to ensure the chairs will not tip over. Our research uses pre-fabrication simulations, but our goal is sensing rather than just ensuring physical properties of the final objects.

    \subsection{Physical Authoring of Digital Models}

    Many researchers have explored physical CAD tools, as this can ideally help those without expertise in using CAD to create or modify objects. Such systems use the link between the physical and digital models in one way or another. Some of this work is done using proxy objects, for example by tracking Duplo blocks stacked by the user \cite{gupta-duplotrack}, capturing annotations on paper versions of models \cite{song-modelcraft-tochi}, or scanning sculpted clay models marked with stickers \cite{savage-mmarks}. Other research explores actually having a designer work with a fabrication tool directly, capturing their toolpath and creating a model and/or final artifact by automatically tidying it up \cite{willis-interactive, mueller-constructable, mueller-laserorigami}. Conversely, the fabrication tools can \emph{direct} a toolpath using information from a 3D model and enable a dialogue between user, tool, and model \cite{zoran-freed}. We similarly link object geometry to its physical form, however we pursue this link for sensing purposes rather than for form finding.

    \subsection{Integrating Electronics and Existing Objects}

    Techniques for creating interactive objects have been widely examined in HCI: allowing designers to integrate electronics hardware or other existing objects into fabricatable designs is a common approach. Sensing modules can be the primary driver of an object's form factor, for example by allowing designers to drag the modules around digitally and re-forming a shape that will hold them all \cite{weichel-enclosed}. This technique could be useful as an add-on for our tools, but we focus on sensing rather than authoring 3D case forms. Pipe structures for joining sensors to microcontroller boards can allow for additional flexibility in design \cite{savage-sot}, however this requires that the designer has intimate knowledge of the sensing techniques to be used. Our tools build in knowledge of the sensing paradigm, relieving that burden from designers and allowing additional redesign flexibility versus one-off, hand-created sensor routings \cite{navarrette-gps, park-microchannels}. Creating an object that fits with existing hardware has been explored using Kinect-based scans of objects \emph{in situ} \cite{molyneaux-kinectfusion, weichel-mixfab} or photographs \cite{lau-modeling}, or intelligent capture of object dimensions with measuring tapes \cite{lee-handscape,weichel-spata}. Our focus is on integrating sensing into a designer's vision for an object: this is a complementary tactic.

\section{Sensing Techniques}

    %\valkyrie{not all of these are for fabbed objects, but a slightly broader swath of sensing techniques definitely needs to be discussed here. segmenting into fabbed vs. not doesn't really draw a bright line of any kind, though, so... how to structure this part?}
    Research has likewise investigated a variety of ways for sensing manually- and digitally-fabricated devices, ranging from switches that close when a flexible object is bent \cite{slyper-structure} to audio frequencies swept through fabricated flutes \cite{laput-acoustruments}.

    \subsubsection{Using machine learning}
        Machine learning provides power and nuance to sensing tasks. By sweeping electrical frequencies through a conductive material (e.g., water), machine learning can detect if a user touches it with a finger versus with two fingers \cite{sato-touche}, or using audio in a similar way can detect palm versus fingertip interactions with objects \cite{ono-touchandactivate}. The acceleration profiles of scratching noises can be detected as gestures \cite{harrison-scratchinput}, and scratching noises along ridges can reveal whether a user is rubbing an object clockwise or counterclockwise \cite{murray-smith-stane}. Simple camera-based and depth-camera-based sensing can likewise be trained to detect user interactions  \cite{fails-crayons,holman-sketchspace,klemmer-papiermache,macintyre-DART}.

        Even more subtle interactions can be captured, for example by using continuous interpolation of signatures. This has been applied to sound swept through 3D printed flute-like tubes \cite{laput-acoustruments} to determine position continuously along sliders.

        However, employing machine learning requires training each gesture to be sensed. In addition, it requires training each \emph{combination} of gestures to be sensed, as swept sound or capacitance signatures may not combine linearly when gestures are performed contemporaneously. Our work eschews this complication: since we link geometry to sensing, we exploit this link to allow training-free sensing and/or improve our chances of sensing a user's interactions correctly.

    \subsubsection{Without machine learning}
        Other explorations have led to clever ways of avoiding the post-fabrication training necessary to use machine learning for sensing. For flexible objects, this can come in the form of internal switches designed to close when cast-silicone objects are manipulated in certain ways \cite{slyper-structure}, or as conductive material within microchannels whose resistance changes when stretched or bent \cite{majidi-curvature, park-microchannels}. Time-domain reflectometry, i.e., sending an electrical pulse through a wire and timing how long it takes to reflect back, can likewise detect user interaction with flexible objects \cite{wimmer-tdr}, as can adhesive sensing tape \cite{holman-tactiletape}. These techniques do not employ digital fabrication: they are hand-fabricated and -tuned. Because we digitally fabricate our artifacts, we can generate knowledge of geometry automatically. Digitally fabricated soft objects can have stretchable electronics embedded afterwards \cite{yao-pneui}, or be sensed using computer vision and barometric pressure \cite{harrison-buttons, slyper-pressure}; however, since these objects do not use knowledge of their interactive pieces, they cannot exploit the geometry-sensing link that we use for our work. Similarly, Olberding, et al., created a sensor that is maximally robust to post-fabrication user modification \cite{olberding-cuttable}. Again, this technique does not exploit the link between an object's geometry and its fabricated form to improve sensing.

        Identity can be embedded in fabricated objects' surface textures (intended for scratching) \cite{harrison-acoustic}; indeed identity can also be recorded in invisible chambers \emph{inside} an object for later high-frequency imaging \cite{willis-infrastructs}. These projects use knowledge of the fabricated, embedded identity tags to ``train'' their sensing, however they can only detect an object's identity and not how a user is interacting with it.

        Actual user interaction with digitally fabricated objects has been explored, as well, most notably by Willis, et al. Printed Optics allows for sensing fabricated mechanisms (like sliders and buttons) via light shining through tiny channels embedded in a print \cite{willis-printedoptics}. This work inspired our own, though Printed Optics does not offer a design tool or a way of providing this sensing without extensive hand-programming.

\section{Prototyping Tools}

    Our investigations in this thesis are ultimately looking at techniques for prototyping. There are many different types of prototypes: typically they are split into role, look-and-feel, and implementation \cite{houde-prototypes}. Within each of these types of prototype, there are opportunities to explore high-fidelity prototypes and low-fidelity prototypes. Our work seeks to help with high-fidelity look-and-feel and role prototypes; we recognize that our sensing techniques have some drawbacks as tools for final implementation (for example, the processing time required to recognize interactions using computer vision or audio).

    Other research has investigated easing this type of physical functionality exploration, often through the creation of toolkits that help users create interactive prototypes quickly. On the low-fidelity end, simple pushpins and copper tape can create circuitry for testing with cardboard mockups \cite{hudson-boxes}, or the TUIs can be ``sketched'' using augmented reality tools \cite{nam-sketchingtuis}. Another simple solution is attaching an accelerometer to an object and using its output to detect interactions \cite{hook-making}. More sophisticated toolkits may include multi-purpose programmable microcontrollers \cite{arduino}, or even be designed for prototyping interactive devices on cloth \cite{buechley-lilypad}. Designers can have extra freedom with placing electronics when a board becomes a smart substrate that interlinks them automatically \cite{villar-voodooio}, or can use snap-together sensing and actuation modules \cite{avrahami-switcharoo, greenberg-phidgets, lee-calder} that could be programmed in a visual language \cite{villar-gadgeteer}. These techniques have several important limitations: most importantly, they \emph{constrain exploration}: if a user wishes to include a 3-inch slider in his design, but the kit only offers a 2-inch slider or a 5-inch slider, he has to make due. Our work's use of digital fabrication for sensor creation gives users this flexibility in design exploration, without tying them to pre-defined form factors.

    Circuitry, and thus any existing electronics, can be integrated directly into a 3D printed object by laying down conductive material \cite{sells-reprap,voxel8,sarik-tracebrush}, or by leaving voids to be filled with conductive material post-print \cite{savage-sot}; this still constrains designers to what already exists. Techniques like adding stick-on sensors or sensing tags \cite{maynes-aminzade-eyepatch,yeo-stickear} can mitigate this challenge, however they lead to one-off prototypes: with digital fabrication, designers can create a design, test it, then modify it digitally to create an improved design; they can even share it with coworkers without rebuilding it by hand each time. A more general purpose sensing \emph{core} can be added to new prototypes, necessitating that only the body of the design change each time \cite{doering-composition}: we build on this idea and leverage our knowledge of an object's geometry to improve it.

    We also examine another important question: how is functionality defined in interactive prototypes? Software can be automated using screenshots as in Sikuli \cite{yeh-sikuli}, or with visual ``block''-based programming languages that are accessible to even children \cite{resnick-scratch}. Programming by demonstration can also aid novices in developing interactive applications with sensor components \cite{hartmann-dtools}, similarly functionality can be inferred from wireframe mockups of applications \cite{li-framewire}. To integrate hardware and software functionality, some researchers have explored augmented reality \cite{nam-AR} or projection \cite{akaoka-displayobjects} techniques to allow interactions to be defined only in code without functional hardware; recent online tools like 123D Circuits allow for simulating hardware and testing code for it without requiring the physical circuit be built \cite{123dcircuits}.

\section{Conclusion}

We have discussed related work from simulation, sensing, 3D modeling, and prototyping tools, and described how our work fits into and links these different research areas. We will now dive into each of our projects.
