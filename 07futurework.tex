\chapter{The Final Word}

\begin{quote}
When I look into the future, it's so bright it burns my eyes.

--- Oprah Winfrey
\end{quote}

This thesis has presented techniques for leveraging and modifying digital geometry of objects to aid in creating fast, cheap, and flexible prototyping tools. We conclude with a discussion of the links between and importance of described projects, as well as pointers to future work.

\section{Discussion of Projects}

The large thrust of this thesis is this: designing tangible input devices is challenging---far more so than creating prototypes of graphical user interfaces---because prototypes must combine software, hardware, and custom enclosures. For this case, we believe digital fabrication can help. With digital fabrication, we have a model of an object \emph{before we have the object itself}, which we can manipulate and simulate in light of the sensing mechanism we plan to use for our final prototype. We call this process, which links fabrication-for-interaction to models-for-simulation, ``Fabbing to Sense."

The three projects we discussed---Midas, Lamello, and Sauron---serve as exemplars in exploring the space of Fabbing to Sense. Between them, they leverage touch, audio, and vision: several very common sensing modalities today. For this thesis, that has the benefit that they are easy to communicate to other researchers. Ultimately, though, designers and makers, and those they create for, do not need to know the workings of the sensing mechanisms.

As with existing graphical user interface toolkits, which can abstract away callbacks, container structures, rendering styles, and other low-level details, we hope to see our techniques and others like them be abstracted into black boxes. As such, they can support designers' high-level goals, like ``button here," or ``continuous volume knob," without requiring additional expertise. From designers' goals, the key pieces are simulation and modification---using knowledge of the sensing technique---to allow final fabrication. We dipped into such high-level specifications in Midas and Sauron, and found that designers were able to create unique input devices using the systems.

Beyond simply supporting design tasks, we see simulation as a way to ensure that unusual sensing techniques can disappear from the perspective of the final end-users of prototype devices. Midas can check for assembly errors to ensure its sensors will behave properly. Sauron ensures that its modifications will not interfere with users' manipulating input components through their full ranges of motion.

It may also be the designer's choice that the sensing mechanism \emph{not} fade into the background. In the Lamello project, we used sounds within typical human hearing ranges as control signals. While some may view this as a downside (and likely one that could be corrected using different materials and/or smaller structures), it is possible for designers to integrate this as a feature: as one example, a slider mechanism could be programmed to play a favorite song as it is manipulated. Thus, depending upon the selected modality, a designer may choose to either hide or accentuate a device's sensing technique.

In any case, integration of physical form-finding with specification of input components can be a challenge. This work is currently relegated to those with expertise in Computer-Aided Design (CAD) tools. While it does not fall under the banner of Fabbing to Sense, we have also done work on allowing users to create such physical specifications that are authored in the real world, using easily reconfigurable materials and leveraging computer analysis, simulation, and optimization \cite{savage-mmarks}. We see this as part of the future ecosystem of Fabbing to Sense.

All of the above is part of a larger trend in computer science, and especially in Human Computer Interaction, whereby computing devices are imbued with domain expertise, and further they are given the ability to assist users via guidance or corrective actions. Such creativity support tools can exploit the strengths of each system. That is, they marry human creativity and perceptive skills with machine precision and simulation. Systems connecting the two have been tested already for use in surgical (e.g., \cite{kahol-surgical}) and cooking (e.g., \cite{sato-mimicook}) environments, as well as innumerable other physical and virtual Augmented Reality (AR) environments for teaching novices or supporting experts. As more systems successfully demonstrate this technique, the human-machine interlink for aiding designers in their creation of tangible input devices represents a natural research direction, as well as a necessary component for designing the future of interaction.

Another piece of the future may be changing some of the constraints for which Fabbing to Sense systems optimize. Multi-material printers can now create conductive traces \cite{voxel8}, but may someday be integrated with pick-and-place machines to allow for fully-integrated electronics in a single pass. When a designer has to perform the assembly, we prefer to have a single sensor that can sense all inputs at once. However, if a machine performs the assembly, future systems could employ mixtures of multiple types of sensing, with automatic optimizations around cost, size, or other design factors. This would allow for further abstraction: designers can stipulate that some inputs should be continuous and others discrete, allowing the system to assign appropriate sensors to each input.

Moving beyond prototypes, Fabbing to Sense could aid designers in creating low-power environmental inputs for long-term use. As mentioned in the Lamello example of a wall-mounted slider, devices which can be sensed through-air by a user-carried sensor need not be continually powered. Most 3D printing processes today create plastic objects that are not suitable for installations such as this---particularly for Lamello-style sensors which subject the devices to significant physical forces in typical use. However, more and better materials are becoming available to those who work with digital fabrication at every level, whether hobbyists or professionals, and the use of metal-based 3D printers or multi-axis CNC mills with sturdier materials may allow for longer-term usefulness of these devices. Aside from materials improvements, another vector by which to approach more viable objects would be constraining sensing: input and sensing modalities which do not require friction, striking, or other continuous contact between fabricated or electronic parts will likely outlast those that do.

\section{Future Work}

Over the course the thesis, we have pointed to a variety of limitations and possible improvements per-project. In general, we see making each project's sensing apparatuses more mobile to be an useful engineering task. All three projects use sensing types common to today's mobile smart phones (capacitive touch, microphone-based audio, and camera-based video), and additional engineering could better leverage those and other sensors already available to designers from within their environment. Some projects already use in-phone multi-touch screens for custom-manufactured capacitive inputs \cite{chan-capstones,chang-clipon}, and the built-in microphone and camera come with easy-access APIs for use as sensors.

Beyond per-project improvements, we have laid out a design space for linking geometry to sensing. Midas, Lamello, and Sauron each represent a single point in this design space, and fuller exploration of the space may lead to companion projects to those described here. In particular, the advent of multi-material PolyJet 3D printers seems to open a wide variety of options for exploring colors, transparency, and flexibility as features in a sensing design space, and the Voxel8 \cite{voxel8} printer, which can lay arbitrary conductive materials and plastic together in a single pass, points to opportunities to explore induction, human body heat sensing, or magnetism as sensing operations. Or, as discussed in Chapter 2, level sensors may pair well with translucent tubing filled with liquids.

Overall, we recognize that there are several assumptions made by the projects presented in this dissertation. Namely, our projects leverage a \emph{single fabrication machine} for creating \emph{one prototype} at a time, which is \emph{hand-optimized} by a designer and sensed by a \emph{single} sensor. We discuss possibilities opened by willfully subverting each of these assumptions in turn.

\subsection{Ecosystems of fabrication machines}

The projects described in this dissertation have largely focused on single fabrication machines working to create a finished prototype object. We also see opportunities for combining the abilities of several machines, whether to speed up the prototyping process or to investigate unique properties that allow exploration of the machines and sensors design space.

    \subsubsection{For celerity}

    While 3D printers allow for near-infinite flexibility in the forms that they are able to create, they still run very slowly. This can be compared to laser cutters, which offer significant speedups in exchange for only producing 2- or 2.5D prototypes (or limited 3D prototypes, see \cite{mueller-laserorigami}). Some prior work has investigated speeding up fabrication through integration of lasercut and 3D printed pieces \cite{beyer-platener}, or use of building blocks with 3D printed parts \cite{mueller-fabrickation}; however, these speedups do not make any use of their knowledge of the completed object post-fabrication. As demonstrated in the Lamello project, lasercut tines integrated with 3D printed bodies allow for faster fabrication with similar accuracy. However, having a designer hand-assemble highly complex input mechanisms fabricated on multiple machines may be prohibitive in terms of time spent: better would be using pre-fabrication simulations to make these devices (partially) self-assembling \cite{tibbits-self-assembly}.

    \subsubsection{For properties}

    Speed is a factor worth considering, but by leveraging multiple machines designers can additionally access a larger variety of properties. Plastic can offer a sturdy base with configurable haptics \cite{torres-hapticprint}, while inkjet-printed circuitry can provide a slide-in base for electronics. Lasercut or cnc-milled wood may pair well with delicate paper to create shape-changing interfaces \cite{yao-pneui}. Leveraging multiple properties (potentially in combination with multiple sensors, as below) may allow for Fabbing to Sense a greater variety of devices. And in some cases, these multi-machine integrations could allow for output in addition to sensing \cite{yao-pneui}.

\subsection{Branching prototypes}

One important benefit of digital models of prototypes is that they become like code: they can be stored, shared, replicated, versioned, and unit-tested. Version-control website github \cite{github} in 2013 added a built-in viewer for STL files in user repositories on the site, offering a powerful tool for those who wished to version their physical designs. However, each design is hand-crafted by the designer. In the future, we would like to explore tools which can create likely \emph{spaces} of prototype designs given an initial seed from a designer, and which then allow testing multiple similar designs in parallel. This could follow prior work on understanding design spaces and how computers can support designer exploration, particularly through parametric modeling \cite{woodbury-designspace, woodbury-parametric}. Given the nature of the sensing performed by the toolkits presented, this type of small multiples testing should be as straightforward as attaching the sensing module to each new prototype.

\subsection{Machine-optimized prototype designs}

While describing a design space and smartly \emph{exploring} points in it to test in parallel could support a designer who has something in mind, another opportunity lies in automatic \emph{generation} of digital interfaces to suit particular people and/or tasks. This is similar in theory to Cogtool \cite{john-cogtool}---which allows designers of web applications to demonstrate tasks for their interfaces, then optimizes the interface to make completion of those tasks as quick as possible for end-users---, and Supple/Supple++ \cite{gajos-supple,gajos-supplepp}---which model users' motor and vision capabilities and automatically adapt their GUI to suit.

We believe that there is significant territory to be explored in modeling users' individual capabilities as relevant to tangible input devices, as well as understanding how to create optimal inputs devices suited to specific tasks.

    \subsubsection{To suit particular users}

    Individual users have wide-ranging abilities and preferences, especially when it comes to something as personal as the hands. In addition, mobility-impaired users may have special requirements for input devices. Traditional mass-produced input devices are designed to be comfortable for $95$\% of target users, however the advent of digital fabrication allows for one-off objects with no startup costs like those associated with tooling in traditional manufacturing. Thus, we can measure the capabilities of a single person (How large are Giorgia's hands? How far can Ethan bend his thumbs? How fast can Shiry pinch her fingers together?), and use our results to design for that person specifically. Such measurements might inform dimensions (e.g., overall size of an input device), locations (e.g., spacing between buttons), or even sensitivity (e.g., matched to user grip strength). This has seen a bit of exploration in the form of anatomical scanning for medical devices \cite{smakman-curatio}, but \emph{motion} is critical for interaction.

    \subsubsection{To suit particular tasks}

    Our lives contain a variety of general-purpose input devices, like the mice and keyboards that we find at our desks, or the video game controllers we use when we unwind. As described above, with digital fabrication machines we can imagine a future in which each input device is uniquely suited to a single task: we can ask questions about a task, and how a particular person approaches it (What abilities does Pat tend to use most when playing League of Legends? How often does Friedhelm scroll through documents while editing them, compared to insert/delete tasks?), and use these to inform the layout (e.g., more frequently-used functions close to dominant hand/fingers) or sensitivity (e.g., a lighter push will activate a time-critical function, versus a more substantial push necessary for an irreversible function) of input devices.

\subsection{Multi-sensor Units}

In the interest of reducing assembly time, all of our projects leverage a \emph{single} sensor in a single location. However, many commercially-available sensor bundles could offer additional types of data. For example, modern smartphones collect a wide array of sensors---e.g., microphone, camera, accelerometer, gyroscope, magnetometer---into a package that could easily be attached to a single point in a prototype; likewise, Texas Instruments' BLE SensorTag sports an ambient light sensor, humidity sensor, barometric pressure sensor, magnetometer, temperature sensor, and more \cite{ti-sensortag}. By leveraging multiple of these sensors in a single prototype, we can create objects which respond to multiple modalities of interaction: for example, supporting the design of objects which integrate squeezable soft components sensed using air pressure, yet can also determine their orientation in space using an accelerometer, allows designers greater flexibility in their process. Modelling potential interference between different sensor types may present an interesting problem here.

\section{Closing Remarks}

As computing moves off the desktop and into the world, we see designers exploring many varieties of input devices to suit new and specific tasks. These new physical devices necessitate a new kind of prototyping, that does not rely fully on \emph{virtual} software but which can help create functional \emph{physical} objects in a fast, cheap, and flexible way. This dissertation has described digital fabrication as a means of accomplishing this: by linking a digital design to a completed physical object, we can shorten the drudgery associated with each iteration cycle by offloading expertise---that previously would have been required of the designer---to her computer. We have presented three examples of this: Midas performed sensor routing and used this knowledge to pre-program its capacitive sensor microcontroller and ensure assembly correctness. Lamello used 3D geometry to predict tines' resonant frequencies for audio sensing. Sauron manipulated an object's digital model to ensure that a single camera would be able to sense all the components inside, as well as to keep pieces from colliding in the hands of an end-user. All three represent the paradigm of ``Fabbing to Sense," which employs knowledge of the sensing technique that will ultimately be used \emph{throughout} the design process, and optimizing prototypes for that technique.

Our suite of tools, and the space that they explore, will hopefully empower designers to invent and hone designs for the future of interaction.
